{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ 원본 : http://cs231n.github.io/classification/\n",
    "\n",
    "이 강의는 컴퓨터 비전 외부의 사람들(잘 모르는 사람들)을 이미지 분류 문제 및 데이터 기반 접근 방식으로 안내하기 위해 설계된 강의입니다.\n",
    "\n",
    "**<목차>**\n",
    "\n",
    "* [Intro to Image Classification, data-driven approach, pipeline](#Image-Classification)\n",
    "* [Nearest Neighbor Classifier](#Nearest-Neighbor-Classifier)\n",
    "    * [k-Nearest Neighbor](#k-Nearest-Neighbor-Classifier)\n",
    "* [Validation sets, Cross-validation, hyperparameter tuning](#Validation-sets-for-Hyperparameter-tuning)\n",
    "* Pros/Cons of Nearest Neighbor\n",
    "* Summary\n",
    "* Summary: Applying kNN in practice\n",
    "* Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "**Motivation**. 이 섹션에서는 입력 이미지를 고정된 카테고리 세트에서 하나에 라벨에 할당하는 작업인 이미지 분류 문제에 대해 소개할 것입니다. 이 것은 단순함에도 불구하고 실용적인 응용 프로그램이 매우 다양한 컴퓨터 비전의 핵심 문제중 하나입니다. 게다가 우리가 과정 뒷 부분에서 보게 될 것처럼, 겉으로 보기에는 별개처럼 보이는 컴퓨터 비전 작업들(object detection, segmentation과 같은)은 이미지 분류로 축소될 수 있습니다.\n",
    "\n",
    "**Example**. 예를 들면, 아래 이미지에서 이미지 분류 모델은 하나의 이미지를 가져와서 4개의 레이블 {cat, dog, hat, mug}에 확률을 지정합니다. 이미지에서 보다싶이, 컴퓨터의 이미지는 하나의 큰 3차원 숫자 배열로 표시됩니다. 이 예에서 고양이 이미지는 너비가 248 픽셀이고 높이가 400 픽셀이며 빨강, 초록, 파랑(RGB)의 세 가지 색상 채널이 있습니다. 따라서 이미지는 248 x 400 x 3의 숫자 또는 총 297,600개의 숫자로 구성됩니다. 각 숫자는 0(black)~255(white) 범위의 정수이다. 우리가 해야 할 작업은 이 297,600개의 숫자를 'cat'과 같은 단일 라벨로 바꾸는 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "![cat](./image/cat.PNG)\n",
    "\n",
    "이미지 분류 작업은 주어진 이미지에서 하나의 라벨(또는 여기에 표시된 것과 같은 라벨의 분포)을 예측하는 것입니다. 이미지는 Width x Height x 3 크기의 0~255 사이 정수의 3차원 배열로 되어있습니다. 3은 빨강, 녹색, 파랑의 세 가지 색상 채널을 나타냅니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges**. 이러한 시각적 개념(e.g. cat)을 인식하는 작업을 인간이 수행하는 것이 상대적으로 사소한 일이기 때문에 컴퓨터 비전 알고리즘 관점에서 제기된 과제를 고려하는 것이 좋습니다. 아래의 도전 과제 목록을 제시할 때, 이미지의 원시 표현을 3차원 밝기 값 배열로 표현하는 것을 명심하십시오.\n",
    "\n",
    "* **Viewpoint variation**. 객체의 단일 인스턴스는 카메라와 관련하여 다양한 방법으로 방향을 지정할 수 있습니다. 한마디로 같은 객체라도 다양한 방향으로 카메라에 찍힐 수 있습니다.\n",
    "* **Scale variation**. 비주얼 클래스는 종종 크기가 다양합니다(실세계에서의 크기 뿐만 아니라 이미지의 범위에서도).\n",
    "* **Deformation**. 많은 관심 대상은 강체가 아니며 극단적인 방향으로 변형될 수 있습니다.\n",
    "* **Occlusion**. 관심 대상은 어떤 물체에 가려질 수 있습니다. 때때로 대상의 작은 부분(적은 수의 픽셀)만이 보여질 수도 있습니다.\n",
    "* **Illumination conditions**. 조명 효과는 픽셀 수준에서 급격한 변화를 만듭니다.\n",
    "* **Background clutter**. 관심 대상은 해당 환경에 섞여 식별하기 어려울 수도 있습니다.\n",
    "* **Intra-class variation**. 관심 대상의 종류는 종종 의자와 같이 상대적으로 광범위할 수 있습니다. 이러한 개체에는 여러 가지 유형이 있으며, 각 개체는 고유한 모양을 갖습니다.\n",
    "\n",
    "좋은 이미지 분류 모델은 이러한 모든 변형의 외적(cross product)에 변하지 말아야 하는 동시에 클래스간 유사도에 대한 민감도를 유지해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "![Challenges](./image/challenges.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data-driven approach**. 이미지를 별개의 카테고리로 분류할 수 있는 알고리즘을 작성하려면 어떻게 해야 할까요? 예를 들어 숫자 리스트를 정렬하는 알고리즘을 작성하는 것과는 달리, 이미지에서 고양이를 식별하는 알고리즘을 작성하는 방법은 명확하지 않습니다. 그러므로 모든 관심 분야가 코드에서 직접적으로 보이는 것을 대신해서, 우리가 취할 접근법은 어린이 같이 접근하는 것입니다: 컴퓨터에 각 클래스의 많은 예제를 제공한 다음, 이 예제를 보고 각 클래스의 시각적 모양에 대해 학습하는 알고리즘을 개발하는 것입니다. 이 접근 방식은 레이블이 지정된 이미지의 트레이닝 데이터셋을 먼저 축적하기 때문에 데이터 기반 접근 방식이라 합니다. 다음은 이러한 데이터 집합의 모습입니다:\n",
    "\n",
    "---\n",
    "\n",
    "![data-driven](./image/data-driven.png)\n",
    "네 가지 시각적 카테고리에 대한 트레이닝 세트이다. 실제로는 수천 개의 카테고리와 수십만 개의 이미지를 가질 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The image classification pipeline**. 이미지 분류 작업은 단일 이미지를 나타내는 픽셀 배열을 가져와서 레이블을 지정하는 것입니다. 완벽한 파이프라인은 다음과 같은 형태를 가질 수 있습니다:\n",
    "\n",
    "* **Input**: 입력은 각각 다른 K개의 클래스 중 하나로 라벨링된 N개의 이미지 세트로 구성됩니다. 이 데이터를 *training set*라고 합니다.\n",
    "* **Learning**: 이 작업은 트레이닝 세트를 사용하여 모든 클래스가 어떻게 생겼는지를 학습하는 것입니다. 이 단계를 *training a classifier* 또는 *learning a model* 이라고 합니다.\n",
    "* **Evaluation**: 결국 classifier의 품질을 평가하기 위해 이전에 한번도 보지 못했던 새로운 이미지 셋의 라벨을 예측하도록 요청합니다. 그런 다음 분류 기준에서 예측된 라벨과 실제 이미지의 라벨을 비교합니다. 직관적으로, 많은 예측이 정답(우리는 진실이라고 부름)과 일치하기를 바랄 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Classifier\n",
    "\n",
    "첫 번째 접근 방식으로, 우리는 **Nearest Neighbor Classifier**를 개발할 것입니다. 이 classifier는 *Convolutional Neural Networks*와는 아무런 관련이 없고 실무에서 사용되는 경우도 거의 없지만, 이미지 분류 문제에 대한 기본적인 접근법에 대한 아이디어를 얻을 수 있습니다.\n",
    "\n",
    "**Example image classification dataset: CIFAR-10**. 연습용 이미지 분류 데이터 세트중 유명한 [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) 데이터 세트가 있습니다. 이 데이터 세트는 32x32픽셀의 작은 이미지 60,000개로 구성되어 있습니다. 각 이미지는 10개의 클래스들(ex. airplane, automobile, bird, etc...) 중 하나로 라벨링 되어 있습니다. 이 60,000개의 이미지들은 50,000개의 training set와 10,000개의 test set로 구분됩니다. 아래의 이미지에서 10개의 클래스에 대한 각각 10개의 임의의 예제 이미지를 볼 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "![Nearest-Neighbor](./image/Nearest_Neighbor.png)\n",
    "좌측: [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) 데이터 세트의 예제 이미지.\n",
    "우측: 첫 번째 열은 몇 가지 테스트 이미지를 보여주며 다른 열들은 픽셀단위 차이에 따라 training set에서 상위 10위의 Nearest Neighbor들을 보여줍니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 CIFAR-10 training set가 50,000개(각 클래스당 5,000개)로 주어졌다고 가정하고, 나머지 10,000개의 test set에 라벨을 지정하려 합니다. Nearest Neighbor classifier는 테스트 이미지를 가져와 트레이닝 이미지의 모든 단일 이미지와 비교하고 가장 가까운 트레이닝 이미지의 라벨일 것이라고 예측합니다. 위 이미지와 오른쪽 이미지에서 10가지 예제 테스트 이미지에 대한 예제 결과를 볼 수 있습니다. 10개의 예제 중 약 3개의 예제에서만 동일한 클래스의 이미지가 검색되고, 약 7개의 이미지는 다른 클래스의 이미지가 검색됩니다. 예를 들어, 8번째 행의 말의 머리 test set 이미지의 가장 유사한 training set 이미지가 빨간 색 자동차 이미지인데, 아마도 검은 색 배경이 크게 작용했을 것입니다. 결과적으로 이 경우에 말 이미지는 자동차로 잘못 표시됩니다.\n",
    "\n",
    "아직까지는 training set과 test set의 두 이미지를 정확하게 비교하는 방법에 대한 세부 사항을 지정하지 않은 채로 남겨두었습니다. 가장 간단한 가능성 중 하나는 이미지를 픽셀 단위로 비교하고 모든 차이를 더하는 것입니다. 다시 말해서, 두 개의 이미지가 주어지고 그것들을 벡터 I1, I2로 표현한다면, 다음에 나오는 **L1 거리**로 그들을 합리적으로 비교할 수 있을 것입니다:\n",
    "\n",
    "![L1거리](./image/L1거리.png)\n",
    "\n",
    "모든 픽셀에서 합계를 구합니다. 다음은 해당 과정을 시각화한 것입니다:\n",
    "\n",
    "---\n",
    "\n",
    "![NN시각화](./image/NN_시각화.png)\n",
    "위 그림은 픽셀 단위의 차를 사용하여 두 이미지의 L1 거리를 구하는 예입니다(이 예에서는 한 색 채널). 두 개의 이미지를 요소별로 뺀 다음 모든 차이를 단일 숫자에 더합니다. 만약 두 이미지가 같다면 결과는 0이 될 것입니다. 그러나 만약 이미지가 매우 다를경우 결과는 아주 크게 나올 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드에서 classifier를 구현하는 방법에 대해서 살펴보겠습니다. 먼저 CIFAR-10 데이터를 *training set/labels*와 *test set/labels* 4개의 배열로 메모리에 로드합니다. 아래에 있는 코드에서 *Xtr*(50,000x32x32x3 사이즈)은 training set의 모든 이미지를 가지고 있고, 1차원 배열 *Ytr*(50,000 사이즈)은 training set에 대응하는 label(0~9)을 가지고 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이 코드는 원문에는 없지만 아래 코드들을 실행하기 위해서 꼭 필요한 함수라서 찾아서 가져왔습니다.\n",
    "# CIFAR10 데이터를 로드하는 함수입니다. 이 부분을 실행하셔야 아래 부분의 코드가 정상적으로 실행될 것입니다.\n",
    "from __future__ import print_function\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import platform\n",
    "\n",
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "    \n",
    "def load_CIFAR_batch(filename):\n",
    "    # load single batch of cifar\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    # load all of cifar\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)    \n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # 제공되는 함수\n",
    "\n",
    "# 모든 이미지를 1차원으로 평평하게 만듬(32x32x3 -> 3072)\n",
    "Xtr_rows = Xtr.reshape(Xtr.shape[0], 32*32*3) # Xtr_row : 50000 x 3072\n",
    "Xte_rows = Xte.reshape(Xte.shape[0], 32*32*3) # Xte_row : 10000 x 3072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 이미지가 행으로 늘어났고, classifier를 훈련하고 평가할 수 있는 방법은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NearestNeighbor() # Nearest Neighbor classifier 클래스를 생성(아래에 코드에 나옴)\n",
    "nn.train(Xtr_rows, Ytr) # 트레이닝 이미지와 라벨을 통해서 classifier를 학습시킴\n",
    "Yte_predict = nn.predict(Xte_rows) # test set의 라벨을 예측\n",
    "\n",
    "# 올바르게 예측된(i.e. 레이블 일치) 이미지의 평균 갯수를 계산하여 분류 정확도로 표시\n",
    "print('accuracy: %f' % (np.mean(Yte_predict == Yte)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 기준으로는 일반적으로 올바른 예측의 비율을 측정하는 **accuracy(정확도)**가 사용됩니다. 구축할 모든 classifier는 공통 API를 충족시켜야 합니다: 데이터와 라벨을 가지고 학습시킬 *train(X,y)* 함수가 있어야 합니다. 내부적으로, 클래스는 라벨로 일종의 모델을 구축하고 어떻게 데이터로부터 예측할 수 있는 지 알아야 합니다. 그리고 새로운 데이터를 받아 라벨을 예측하는 *predict(X)* 함수가 있어야 합니다. 물론 위의 코드에서는 실제 classifier(NearestNeighbor 클래스)가 생략되었습니다. 다음 코드에 이 템플릿을 만족하는 L1 거리를 가진 간단한 Nearest Neighbor classifier를 구현하였습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NearestNeighbor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        # Nearest Neighbor classifier는 단순히 모든 training data를 저장함\n",
    "        self.Xtr = X\n",
    "        self.ytr = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        num_test = X.shape[0]\n",
    "        \n",
    "        # 출력 유형이 입력 유형과 일치하는 지 확인\n",
    "        Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n",
    "        \n",
    "        # 모든 테스트 행을 반복\n",
    "        for i in range(num_test): # 원문에는 xrange라 되있는데 Python 3.5에서 작동을 안하여 range로 변경\n",
    "            # L1 거리(모든 차를 더함)를 사용하여 i번째 test image에 가장 가까운 training image 찾기\n",
    "            distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n",
    "            min_index = np.argmin(distances) # 거리가 가장 작은 인덱스 얻기\n",
    "            Ypred[i] = self.ytr[min_index] # 가장 가까운 예제의 라벨 예측\n",
    "            \n",
    "        return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드를 실행하면 이 classifier가 CIFAR-10에서 정확도가 **38.6%**밖에 안된다는 것을 알 수 있습니다. 이는 무작위로 추측하는 것보다는 더 낫습니다(10개의 클래스가 있기 때문에 무작위는 10%의 정확도를 가짐). 그러나 인간의 성능(인간의 성능은 [약 94%](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/)로 추정)이나 인간의 정확도와 비슷한 Convolutional Neural Networks(약 95%의 정확도)와 비교해볼 때 정확도가 현저히 떨어집니다(CIFAR-10의 최근 Kaggle competition [리더보드](http://www.kaggle.com/c/cifar-10/leaderboard) 참조).\n",
    "\n",
    "**The choice of distance**. 벡터 사이의 거리를 측정하는 방법은 L1 거리 말고도 다른 많은 방법이 있습니다. 또 다른 일반적인 선택은 두 벡터 사이의 유클리드 거리를 계산하는 기하학적 해석을 가지는 **L2 거리**를 사용하는 것입니다. 다음은 L2 거리의 식입니다:\n",
    "\n",
    "![L2거리](./image/L2거리.png)\n",
    "\n",
    "다른 말로 하면, 이전의 L1 거리와 마찬가지로 픽셀단위의 차이를 계산할 것입니다. 그러나 이번에는 모든 차를 제곱하여 더하고 마지막으로 제곱근을 취합니다. Numpy에서는 위의 코드에서 한 줄만 바꿔서 사용하면 됩니다. L2 거리를 계산하는 코드 라인은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이거만 실행하면 안됨. 위의 코드에서 해당 부분을 바꿔서 통째로 실행해야함\n",
    "distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서는 np.sqrt 함수를 호출했지만, 실제 Nearest Neighbor에서는 제곱근이 단조 함수이기 때문에 제곱근 연산을 생략할 수 있습니다. 즉, 제곱근 연산은 거리의 절대 크기를 조절하지만 순서는 유지되므로 Nearest Neighbor에서 제곱근은 있거나 없거나 상관이 없습니다. 이 코드를 실행하면 이 classifier가 CIFAR-10에서 정확도가 **35.4%**(L1 거리보다 낮은 결과)라는 것을 알 수 있습니다.\n",
    "\n",
    "**L1 vs. L2**. 두 지표간의 차이점을 고려하는 것은 흥미로운 일입니다. 특히 L2 거리는 두 벡터간의 차이에 관해서 L1 거리보다 훨씬 안좋은 결과를 나타냈습니다. 즉 L2 거리는 하나의 큰 차이가 있는 것보다 적당한 여러 개의 차이가 있는 것을 선호합니다. L1과 L2 거리(또는 두 이미지간의 차이에 대한 L1/L2 표준)는 [p-norm](http://planetmath.org/vectorpnorm)에 가장 일반적으로 사용됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbor Classifier\n",
    "\n",
    "예측을 할 때 가장 가까운 이미지의 레이블만 사용하는 것이 이상하다고 눈치챘을 것입니다. 실제로 **k-Nearest Neighbor Classifier**를 사용하면 더 잘할 수 있습니다. 아이디어는 간단합니다: 트레이닝 셋의 이미지 중 가장 가까운 하나의 이미지를 찾는 것이 아니라, 상위 **k**개의 가까운 이미지를 찾고 테스트 이미지의 라벨에 투표하도록 합니다. k=1이라면 Nearest Neighbor과 같은 결과를 나타낼 것입니다. 직관적으로 k의 값이 높을수록 classifier는 이상한 값에 대한 내성이 강해질 것입니다:\n",
    "\n",
    "---\n",
    "\n",
    "![knn](./image/knn.png)\n",
    "위 그림은 2차원 점과 3차원 클래스(Red, Green, Blue)를 사용한 Nearest Neighbor과 5-Nearest Neighbor classifier의 차이점에 대한 예입니다. 색깔 영역은 L2 거리를 갖는 classifier에 의해 생성된 결정 경계입니다. 흰색 영역은 애매하게 분류된 점을 나타냅니다(즉, 적어도 두 개의 클래스의 투표 결과가 동등하게 나오는 경우). NN classifier의 경우 점이 이상하게 찍혀있는 경우(예: 파란 점 중간에 초록 점이 찍힌 경우) 잘못된 예측 가능성이 있는 작은 섬을 만들고, 5-NN classifier의 경우는 이러한 불규칙성을 부드럽게 하여 테스트 데이터에 대한 더 나은 **일반화**를 이끌어낼 수 있습니다. 또한 5-NN 이미지의 회색(색이 없는 부분) 영역은 nearest neighbor(예: 2개의 neighbor가 빨간색, 2개의 neighbor가 파란색, 하나의 neighbor가 초록색) 투표에 의해 발생합니다.\n",
    "\n",
    "---\n",
    "\n",
    "실제로는 거의 항상 k-Nearest Neighbor를 사용하려고 합니다. 어떤 k값을 사용해야 되는지에 대한 문제는 다음에서 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation sets for Hyperparameter tuning\n",
    "\n",
    "k-nearest neighbor classifier는 k에 대한 설정을 필요로 합니다. 과연 어떤 k가 가장 잘 동작할까요? 게다가 거리 계산 방식도 L1 norm, L2 norm 및 우리가 고려하지 않은 많은 거리 계산방식(예: dot product)이 있습니다. 이러한 선택을 **hyperparameter**라고 하며 데이터를 통해서 학습하는 많은 기계학습 알고리즘의 설계에 자주 등장합니다. 어떤 값/세팅을 선택할 지는 때때로 명확하지 않습니다.\n",
    "\n",
    "우리는 더 많은 다른 값들을 시도하고 가장 잘 작동하는 것을 확인하여 그에 대한 hyperparameter를 얻고 싶을 것입니다. 그것은 훌륭한 아이디어이며 우리가 해야할 일이기는 합니다. 그러나 이것은 매우 조심스럽게 이루어져야합니다. 특히, hyperparameter를 조정할 목적으로 test set을 사용할 수 없습니다. 머신 러닝 알고리즘을 설계할 때 test set를 매우 귀중한 리소스라고 생각해야 합니다. test set은 학습이 끝날때 까지는 절대로 만지면 안됩니다. test set에서 제대로 작동하도록 hyperparameter를 조정한다면, 모델을 배포할 경우 성능이 크게 저하될 수 있어서 매우 위험합니다. 이런 경우를 test set에 너무 **overfit** 했다고 말합니다. 또한 test set을 hyperparameter 조정에 사용한다는 것은 test set을 training set으로 사용한다는 것이므로 모델을 만들 때 너무 낙관적으로 만들어집니다(한마디로 test set이 이미 training set에 포함되어 있으니 답을 알고있는 문제를 풀 때처럼 test할 때는 아주 좋은 결과가 나오나, 실제로 사용할 때에는 전혀 다른 안좋은 결과가 나옵니다). 그러나 test set을 마지막에 한번만 사용한다면 classifier가 더 좋은 일반화를 할 수 있습니다(나중에 클래스에서 일반화를 둘러싼 더 많은 논의가 나올 것입니다).\n",
    "\n",
    "> **Test set은 마지막에 모델 평가를 위해 딱 한번만 사용하세요!!**\n",
    "\n",
    "다행히도 test set를 건들이지 않고 hyperparameter를 조정하는 정확한 방법이 있습니다. 바로 training set를 약간 더 작은 training set와 **validation set** 두 개로 나누는 방법입니다. CIFAR-10을 예로 들자면, 50,000개의 training set를 49,000개의 약간 더 작은 training set와 1,000개의 validation set로 나눕니다. 이 validation set은 기본적으로 hyperparameter를 조정하기 위한 거짓 test set으로 사용됩니다.\n",
    "\n",
    "CIFAR-10의 경우 다음과 같이 표시됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이전과 같이 Xtr_rows, Ytr, Xte_rows, Yte가 있다고 가정\n",
    "# Xtr_rows는 50,000 x 3072 행렬임\n",
    "Xval_rows = Xtr_rows[:1000, :] # 첫 1,000개를 validation set으로 지정\n",
    "Yval = Ytr[:1000]\n",
    "Xtr_rows = Xtr_rows[1000:, :] # 나머지 49,000개를 training set으로 지정\n",
    "Ytr = Ytr[1000:]\n",
    "\n",
    "# validation set에 맞게 잘 동작하는 hyperparameter를 구함\n",
    "validation_accuracies = []\n",
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "    # 특정 값의 k와 검증 데이터에 대한 평가를 사용\n",
    "    nn = NearestNeighbor()\n",
    "    nn.train(Xtr_rows, Ytr)\n",
    "    \n",
    "    # k를 입력으로 취할 수 있는 수정된 NearestNeighbor 클래스를 가정\n",
    "    Yval_predict = nn.predict(Xval_rows, k = k)\n",
    "    acc = np.mean(Yval_predict == Yval)\n",
    "    print('accuracy: %f' % (acc,))\n",
    "    \n",
    "    # validation set에서 작동하는 것을 추적\n",
    "    validation_accuracies.append((k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 과정이 끝나면 k의 어떤 값이 가장 좋은 지를 보여주는 그래프를 그릴 수 있습니다. 그런 다음 이 값을 토대로 실제 test set를 한번 평가합니다.\n",
    "\n",
    "> **training set를 training set와 validation set로 나눕니다. validation set를 사용하여 hyperparameter를 조정합니다. 마지막에 test set를 한번 실행하여 성능을 평가합니다.**\n",
    "\n",
    "**Cross-validation**. training data(and validation data)의 크기가 작은 경우, **cross-validation(교차 검증)**이라는 hyperparameter 조정을 위한 더욱 정교한 기술을 사용하기도 합니다. 이전 예에서 작업했던 방법(validation set 나누기)에서 1,000개의 validation set을 임의로 선택하여 나온 validation set를 반복하여 hyperparameter를 계산한다면, 이들 사이 성능의 평균을 구하여 k값이 얼마나 잘 작동하는 지를 더 잘 예측할 수 있습니다. 예를 들어, 5-fold 교차 겁증에서 training data를 5개의 동일한 fold로 분할하고, 그 중 4개를 training set으로, 1개를 validation set으로 사용합니다. 총 5개의 fold에서 validation fold를 돌아가면서 설정하여 성능을 평가하고, 마지막으로 5가지 경우의 성능의 평균을 계산합니다(역자주: 1|2|3|4|5 fold가 있다면 처음은 1을 validation set로 2|3|4|5를 training set로 정하여 hyperparameter를 구합니다. 그 다음은 2를 validation set으로, 1|3|4|5를 training set로 정하여 hyperparameter를 구하고, 이 작업을 5번 반복합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
