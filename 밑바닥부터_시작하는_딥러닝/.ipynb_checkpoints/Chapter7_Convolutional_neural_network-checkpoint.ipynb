{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chapter7. 합성곱 신경방(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현\n",
    "* filter_num : 필터수\n",
    "* filter_size : 필터사이즈\n",
    "*\n",
    "*\n",
    "*\n",
    "*\n",
    "* weight_init_std : 초기화 때의 가중치 초기 표준편차\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 실행코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3002128278\n",
      "=== epoch:1, train acc:0.2679, test acc:0.2572 ===\n",
      "train loss:2.29815949978\n",
      "train loss:2.29498954562\n",
      "train loss:2.28846875312\n",
      "train loss:2.28552308324\n",
      "train loss:2.27189952304\n",
      "train loss:2.25896402054\n",
      "train loss:2.25629842933\n",
      "train loss:2.22734698672\n",
      "train loss:2.1850987856\n",
      "train loss:2.16383642327\n",
      "train loss:2.11617502304\n",
      "train loss:2.07242906391\n",
      "train loss:2.06338961451\n",
      "train loss:1.95600609156\n",
      "train loss:1.92739461371\n",
      "train loss:1.91972703941\n",
      "train loss:1.79688182472\n",
      "train loss:1.75149695139\n",
      "train loss:1.66016201322\n",
      "train loss:1.50342660152\n",
      "train loss:1.53514796227\n",
      "train loss:1.43402933739\n",
      "train loss:1.33331561934\n",
      "train loss:1.37982873438\n",
      "train loss:1.23552669681\n",
      "train loss:1.14954827926\n",
      "train loss:0.966318790133\n",
      "train loss:1.0672653144\n",
      "train loss:0.998089233778\n",
      "train loss:0.952236139593\n",
      "train loss:0.856143961333\n",
      "train loss:0.795819507765\n",
      "train loss:0.778462655624\n",
      "train loss:0.837478597424\n",
      "train loss:0.794489238373\n",
      "train loss:0.696991811395\n",
      "train loss:0.640641676144\n",
      "train loss:1.03146385488\n",
      "train loss:0.559428863561\n",
      "train loss:0.76936820521\n",
      "train loss:0.605738592458\n",
      "train loss:0.586002210876\n",
      "train loss:0.776849973457\n",
      "train loss:0.704888012444\n",
      "train loss:0.502848617161\n",
      "train loss:0.664873228531\n",
      "train loss:0.709632025908\n",
      "train loss:0.583421745831\n",
      "train loss:0.652939571692\n",
      "train loss:0.721141863654\n",
      "train loss:0.508139587623\n",
      "train loss:0.722354526517\n",
      "train loss:0.57389800101\n",
      "train loss:0.574061785541\n",
      "train loss:0.589515328095\n",
      "train loss:0.529210254296\n",
      "train loss:0.42113817876\n",
      "train loss:0.584535817445\n",
      "train loss:0.338683843187\n",
      "train loss:0.477476574661\n",
      "train loss:0.34358592497\n",
      "train loss:0.469677610116\n",
      "train loss:0.654003322993\n",
      "train loss:0.469225262666\n",
      "train loss:0.441156210353\n",
      "train loss:0.439241078799\n",
      "train loss:0.400884416081\n",
      "train loss:0.418056256505\n",
      "train loss:0.536011689196\n",
      "train loss:0.501960871421\n",
      "train loss:0.373383837182\n",
      "train loss:0.457121468455\n",
      "train loss:0.443108673589\n",
      "train loss:0.459407205027\n",
      "train loss:0.313599389124\n",
      "train loss:0.473733196133\n",
      "train loss:0.437081937089\n",
      "train loss:0.429639588568\n",
      "train loss:0.333445174243\n",
      "train loss:0.24184772891\n",
      "train loss:0.339562384525\n",
      "train loss:0.478551336123\n",
      "train loss:0.322208896161\n",
      "train loss:0.381176714187\n",
      "train loss:0.286911949756\n",
      "train loss:0.535898350854\n",
      "train loss:0.49479434445\n",
      "train loss:0.43663970916\n",
      "train loss:0.388838344707\n",
      "train loss:0.360794921965\n",
      "train loss:0.294809366108\n",
      "train loss:0.243114201271\n",
      "train loss:0.307622527901\n",
      "train loss:0.491856495046\n",
      "train loss:0.235466864805\n",
      "train loss:0.565971014588\n",
      "train loss:0.431242226069\n",
      "train loss:0.34960099121\n",
      "train loss:0.411749405087\n",
      "train loss:0.443234140664\n",
      "train loss:0.334593576383\n",
      "train loss:0.315904359035\n",
      "train loss:0.403715388416\n",
      "train loss:0.335015396295\n",
      "train loss:0.551816066768\n",
      "train loss:0.40247458295\n",
      "train loss:0.438679674676\n",
      "train loss:0.383263400163\n",
      "train loss:0.162460032633\n",
      "train loss:0.43164241197\n",
      "train loss:0.388236937714\n",
      "train loss:0.261977850273\n",
      "train loss:0.386251112092\n",
      "train loss:0.40745971837\n",
      "train loss:0.469426886036\n",
      "train loss:0.318925941529\n",
      "train loss:0.272006133271\n",
      "train loss:0.263390648722\n",
      "train loss:0.542530827713\n",
      "train loss:0.371971361199\n",
      "train loss:0.259962947946\n",
      "train loss:0.560892318355\n",
      "train loss:0.323138164245\n",
      "train loss:0.348797393289\n",
      "train loss:0.243144220325\n",
      "train loss:0.328336782178\n",
      "train loss:0.229512568667\n",
      "train loss:0.390848014548\n",
      "train loss:0.464641051225\n",
      "train loss:0.323936634333\n",
      "train loss:0.421876983571\n",
      "train loss:0.366562126769\n",
      "train loss:0.30838117067\n",
      "train loss:0.378958706936\n",
      "train loss:0.271264739743\n",
      "train loss:0.309277224178\n",
      "train loss:0.380696653864\n",
      "train loss:0.25287779946\n",
      "train loss:0.313058761536\n",
      "train loss:0.282484625647\n",
      "train loss:0.298851626132\n",
      "train loss:0.265124067151\n",
      "train loss:0.359114965171\n",
      "train loss:0.232096510524\n",
      "train loss:0.230153005508\n",
      "train loss:0.393903407154\n",
      "train loss:0.224821619381\n",
      "train loss:0.441534963618\n",
      "train loss:0.22731593809\n",
      "train loss:0.491074856257\n",
      "train loss:0.244594789398\n",
      "train loss:0.208130485087\n",
      "train loss:0.230665512143\n",
      "train loss:0.284172845899\n",
      "train loss:0.328908090338\n",
      "train loss:0.374147541672\n",
      "train loss:0.294331891134\n",
      "train loss:0.340722446843\n",
      "train loss:0.319019744602\n",
      "train loss:0.265232191496\n",
      "train loss:0.249954311662\n",
      "train loss:0.265808724465\n",
      "train loss:0.41214047825\n",
      "train loss:0.23910678446\n",
      "train loss:0.208565817549\n",
      "train loss:0.289615784958\n",
      "train loss:0.351681163656\n",
      "train loss:0.271910836046\n",
      "train loss:0.277500966264\n",
      "train loss:0.413469651803\n",
      "train loss:0.244002465152\n",
      "train loss:0.221911172227\n",
      "train loss:0.391391179301\n",
      "train loss:0.317850522597\n",
      "train loss:0.129662562846\n",
      "train loss:0.330667989076\n",
      "train loss:0.244562649311\n",
      "train loss:0.349542989864\n",
      "train loss:0.165992026683\n",
      "train loss:0.270519439343\n",
      "train loss:0.340938186255\n",
      "train loss:0.291393382876\n",
      "train loss:0.325231651211\n",
      "train loss:0.202426032194\n",
      "train loss:0.287831941864\n",
      "train loss:0.328622954918\n",
      "train loss:0.314889932719\n",
      "train loss:0.188036946221\n",
      "train loss:0.107115447228\n",
      "train loss:0.154725747552\n",
      "train loss:0.303208002692\n",
      "train loss:0.151690190223\n",
      "train loss:0.392050441847\n",
      "train loss:0.233317423369\n",
      "train loss:0.253781942097\n",
      "train loss:0.162044009645\n",
      "train loss:0.142208406499\n",
      "train loss:0.317354800137\n",
      "train loss:0.318298107698\n",
      "train loss:0.214923381311\n",
      "train loss:0.270345653048\n",
      "train loss:0.222036180081\n",
      "train loss:0.287006316991\n",
      "train loss:0.270894609905\n",
      "train loss:0.181248123109\n",
      "train loss:0.264485638594\n",
      "train loss:0.258480344666\n",
      "train loss:0.262441169367\n",
      "train loss:0.186176885772\n",
      "train loss:0.246640199199\n",
      "train loss:0.26144763097\n",
      "train loss:0.266885986559\n",
      "train loss:0.296395628108\n",
      "train loss:0.289143655277\n",
      "train loss:0.307025186216\n",
      "train loss:0.205766985366\n",
      "train loss:0.269852407376\n",
      "train loss:0.142740770148\n",
      "train loss:0.234960909189\n",
      "train loss:0.124135977353\n",
      "train loss:0.277769599247\n",
      "train loss:0.186976922921\n",
      "train loss:0.359799256996\n",
      "train loss:0.143287174515\n",
      "train loss:0.265776566474\n",
      "train loss:0.17026674449\n",
      "train loss:0.335444472857\n",
      "train loss:0.210054239643\n",
      "train loss:0.278911599656\n",
      "train loss:0.237230075702\n",
      "train loss:0.286418055029\n",
      "train loss:0.18467917263\n",
      "train loss:0.262277472323\n",
      "train loss:0.161143541617\n",
      "train loss:0.362726534095\n",
      "train loss:0.198187697344\n",
      "train loss:0.224595547483\n",
      "train loss:0.466950402244\n",
      "train loss:0.354995500955\n",
      "train loss:0.326174890414\n",
      "train loss:0.258196605628\n",
      "train loss:0.23827385413\n",
      "train loss:0.256651170322\n",
      "train loss:0.140000453504\n",
      "train loss:0.276179696512\n",
      "train loss:0.298697671378\n",
      "train loss:0.212936766465\n",
      "train loss:0.19262287983\n",
      "train loss:0.27886502858\n",
      "train loss:0.174650083502\n",
      "train loss:0.298698666201\n",
      "train loss:0.322684053111\n",
      "train loss:0.301928490658\n",
      "train loss:0.158279588093\n",
      "train loss:0.176571847384\n",
      "train loss:0.152516968139\n",
      "train loss:0.308909564128\n",
      "train loss:0.310711596288\n",
      "train loss:0.223949431286\n",
      "train loss:0.368551176267\n",
      "train loss:0.219181282457\n",
      "train loss:0.305154513299\n",
      "train loss:0.242676626954\n",
      "train loss:0.103552715861\n",
      "train loss:0.212967256751\n",
      "train loss:0.293202245847\n",
      "train loss:0.178200135278\n",
      "train loss:0.139059259966\n",
      "train loss:0.108555699514\n",
      "train loss:0.258364071719\n",
      "train loss:0.267382523572\n",
      "train loss:0.202591976236\n",
      "train loss:0.261516869597\n",
      "train loss:0.17623932434\n",
      "train loss:0.208561972399\n",
      "train loss:0.200805446435\n",
      "train loss:0.206476066211\n",
      "train loss:0.235745731574\n",
      "train loss:0.33134784646\n",
      "train loss:0.247606833301\n",
      "train loss:0.139687422669\n",
      "train loss:0.15983390649\n",
      "train loss:0.197697392743\n",
      "train loss:0.145130383083\n",
      "train loss:0.0933491857345\n",
      "train loss:0.178094613613\n",
      "train loss:0.256619617219\n",
      "train loss:0.134588444273\n",
      "train loss:0.24944943581\n",
      "train loss:0.262604233999\n",
      "train loss:0.256809930086\n",
      "train loss:0.284376420777\n",
      "train loss:0.168068906128\n",
      "train loss:0.171359668527\n",
      "train loss:0.289243414765\n",
      "train loss:0.166493953656\n",
      "train loss:0.178577737235\n",
      "train loss:0.11587238395\n",
      "train loss:0.227371786371\n",
      "train loss:0.215700290059\n",
      "train loss:0.182470462617\n",
      "train loss:0.217190820931\n",
      "train loss:0.0975228098245\n",
      "train loss:0.202197014317\n",
      "train loss:0.0888755736228\n",
      "train loss:0.197352116357\n",
      "train loss:0.156282790323\n",
      "train loss:0.117860772082\n",
      "train loss:0.19806956656\n",
      "train loss:0.338550282058\n",
      "train loss:0.136513692555\n",
      "train loss:0.265772786307\n",
      "train loss:0.278765911281\n",
      "train loss:0.24167212436\n",
      "train loss:0.322854493633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.130381014611\n",
      "train loss:0.182447941139\n",
      "train loss:0.176095563993\n",
      "train loss:0.272944522459\n",
      "train loss:0.16406618227\n",
      "train loss:0.188147604627\n",
      "train loss:0.173988662419\n",
      "train loss:0.161926357653\n",
      "train loss:0.0827945645132\n",
      "train loss:0.171275104769\n",
      "train loss:0.137053336325\n",
      "train loss:0.326543807929\n",
      "train loss:0.167200191605\n",
      "train loss:0.100787600951\n",
      "train loss:0.167878543475\n",
      "train loss:0.193651124203\n",
      "train loss:0.118592265274\n",
      "train loss:0.108072107817\n",
      "train loss:0.114822556084\n",
      "train loss:0.161812754743\n",
      "train loss:0.198601586826\n",
      "train loss:0.221674767693\n",
      "train loss:0.194738910536\n",
      "train loss:0.117516786875\n",
      "train loss:0.18136440556\n",
      "train loss:0.171605228019\n",
      "train loss:0.16805896987\n",
      "train loss:0.101713367725\n",
      "train loss:0.302243452083\n",
      "train loss:0.113556210818\n",
      "train loss:0.146305706488\n",
      "train loss:0.183229157143\n",
      "train loss:0.191984028646\n",
      "train loss:0.244097603649\n",
      "train loss:0.259831833544\n",
      "train loss:0.211124345014\n",
      "train loss:0.137829094841\n",
      "train loss:0.200477490766\n",
      "train loss:0.147086973119\n",
      "train loss:0.0677854168518\n",
      "train loss:0.131905344638\n",
      "train loss:0.166356710599\n",
      "train loss:0.103531163916\n",
      "train loss:0.279303658381\n",
      "train loss:0.109244556932\n",
      "train loss:0.176646811204\n",
      "train loss:0.242019791237\n",
      "train loss:0.323482003966\n",
      "train loss:0.214950366902\n",
      "train loss:0.168031351069\n",
      "train loss:0.195509363697\n",
      "train loss:0.122324449818\n",
      "train loss:0.192933769301\n",
      "train loss:0.141555739843\n",
      "train loss:0.177383102044\n",
      "train loss:0.0807643549386\n",
      "train loss:0.272709317458\n",
      "train loss:0.126905734683\n",
      "train loss:0.233324117908\n",
      "train loss:0.116080910854\n",
      "train loss:0.097237953279\n",
      "train loss:0.180082823274\n",
      "train loss:0.0801014967821\n",
      "train loss:0.231385551204\n",
      "train loss:0.149952985667\n",
      "train loss:0.211146050599\n",
      "train loss:0.153626168022\n",
      "train loss:0.157861124629\n",
      "train loss:0.207293609994\n",
      "train loss:0.101222727716\n",
      "train loss:0.170421841243\n",
      "train loss:0.112728817691\n",
      "train loss:0.0462882416517\n",
      "train loss:0.203991620456\n",
      "train loss:0.155927373001\n",
      "train loss:0.185780709543\n",
      "train loss:0.0959937198336\n",
      "train loss:0.09750992058\n",
      "train loss:0.177248284449\n",
      "train loss:0.159232593732\n",
      "train loss:0.075278014416\n",
      "train loss:0.315678860486\n",
      "train loss:0.154623652196\n",
      "train loss:0.190079075061\n",
      "train loss:0.111692354961\n",
      "train loss:0.117617880457\n",
      "train loss:0.113794701652\n",
      "train loss:0.23630344836\n",
      "train loss:0.224220323766\n",
      "train loss:0.14003991484\n",
      "train loss:0.184503961563\n",
      "train loss:0.0938724412576\n",
      "train loss:0.226962206342\n",
      "train loss:0.204291824014\n",
      "train loss:0.163819305299\n",
      "train loss:0.154120752434\n",
      "train loss:0.130329378459\n",
      "train loss:0.287526616648\n",
      "train loss:0.148832370146\n",
      "train loss:0.199927380091\n",
      "train loss:0.118373244934\n",
      "train loss:0.23368457395\n",
      "train loss:0.210565929555\n",
      "train loss:0.247750951629\n",
      "train loss:0.140890961123\n",
      "train loss:0.121658628525\n",
      "train loss:0.222588052626\n",
      "train loss:0.0806297035969\n",
      "train loss:0.191054552551\n",
      "train loss:0.117617040416\n",
      "train loss:0.0751422748496\n",
      "train loss:0.123812965922\n",
      "train loss:0.179229451618\n",
      "train loss:0.206185246054\n",
      "train loss:0.0601241335071\n",
      "train loss:0.200750366184\n",
      "train loss:0.0853147269278\n",
      "train loss:0.0632972230232\n",
      "train loss:0.185030992024\n",
      "train loss:0.164129289331\n",
      "train loss:0.165891877225\n",
      "train loss:0.0488824909337\n",
      "train loss:0.11378334722\n",
      "train loss:0.151669599599\n",
      "train loss:0.137068540897\n",
      "train loss:0.149799696935\n",
      "train loss:0.118544663875\n",
      "train loss:0.147239450237\n",
      "train loss:0.26886284424\n",
      "train loss:0.152689722475\n",
      "train loss:0.225838823014\n",
      "train loss:0.365572980048\n",
      "train loss:0.0854732205551\n",
      "train loss:0.0781060935541\n",
      "train loss:0.189730404722\n",
      "train loss:0.0597983433695\n",
      "train loss:0.0887936871527\n",
      "train loss:0.0953170681095\n",
      "train loss:0.140362060506\n",
      "train loss:0.0996043267712\n",
      "train loss:0.136539044827\n",
      "train loss:0.11128068582\n",
      "train loss:0.247947973765\n",
      "train loss:0.068680675902\n",
      "train loss:0.0821181656715\n",
      "train loss:0.0818567833199\n",
      "train loss:0.191564834443\n",
      "train loss:0.290799264474\n",
      "train loss:0.163740921609\n",
      "train loss:0.138535848296\n",
      "train loss:0.0883085199005\n",
      "train loss:0.15825213337\n",
      "train loss:0.157090984396\n",
      "train loss:0.13234572182\n",
      "train loss:0.0751611613583\n",
      "train loss:0.163907569641\n",
      "train loss:0.0838461754425\n",
      "train loss:0.129043997008\n",
      "train loss:0.188843096849\n",
      "train loss:0.0873290867507\n",
      "train loss:0.142965536552\n",
      "train loss:0.0836188675579\n",
      "train loss:0.159804017437\n",
      "train loss:0.115845099176\n",
      "train loss:0.282739522537\n",
      "train loss:0.0888504617162\n",
      "train loss:0.122475161686\n",
      "train loss:0.0831746443337\n",
      "train loss:0.19019235538\n",
      "train loss:0.0950851142136\n",
      "train loss:0.0787597283948\n",
      "train loss:0.166824570798\n",
      "train loss:0.209284635175\n",
      "train loss:0.155730737291\n",
      "train loss:0.16827829921\n",
      "train loss:0.0684598664988\n",
      "train loss:0.117424142019\n",
      "train loss:0.152603047655\n",
      "train loss:0.0897792693875\n",
      "train loss:0.143290495407\n",
      "train loss:0.167981861322\n",
      "train loss:0.108545582978\n",
      "train loss:0.0660427005874\n",
      "train loss:0.0610751662108\n",
      "train loss:0.097641440115\n",
      "train loss:0.112568062223\n",
      "train loss:0.185274038019\n",
      "train loss:0.176997413651\n",
      "train loss:0.12290301201\n",
      "train loss:0.216484582759\n",
      "train loss:0.119209532848\n",
      "train loss:0.0561774323402\n",
      "train loss:0.159807054492\n",
      "train loss:0.0273961563712\n",
      "train loss:0.138810289079\n",
      "train loss:0.0928356077073\n",
      "train loss:0.0908604920909\n",
      "train loss:0.147395681724\n",
      "train loss:0.099737561496\n",
      "train loss:0.140435861273\n",
      "train loss:0.136272669583\n",
      "train loss:0.112853516415\n",
      "train loss:0.104154645204\n",
      "train loss:0.159140632006\n",
      "train loss:0.166115651809\n",
      "train loss:0.088308874126\n",
      "train loss:0.125517268656\n",
      "train loss:0.0325674918066\n",
      "train loss:0.246950917235\n",
      "train loss:0.154578336589\n",
      "train loss:0.0446829470046\n",
      "train loss:0.123902921165\n",
      "train loss:0.0676233601641\n",
      "train loss:0.167717355083\n",
      "train loss:0.188348683449\n",
      "train loss:0.196682914363\n",
      "train loss:0.075208077632\n",
      "train loss:0.12374601807\n",
      "train loss:0.10392725803\n",
      "train loss:0.0762970783709\n",
      "train loss:0.161643452744\n",
      "train loss:0.208883798946\n",
      "train loss:0.119318214639\n",
      "train loss:0.0625824739739\n",
      "train loss:0.222137217048\n",
      "train loss:0.0956883227325\n",
      "train loss:0.132621712544\n",
      "train loss:0.173963910936\n",
      "train loss:0.152660640609\n",
      "train loss:0.170245440429\n",
      "train loss:0.111573315949\n",
      "train loss:0.0925870977487\n",
      "train loss:0.223280646195\n",
      "train loss:0.105792094865\n",
      "train loss:0.118125280519\n",
      "train loss:0.087117871131\n",
      "train loss:0.0847960446687\n",
      "train loss:0.181545765546\n",
      "train loss:0.114357289344\n",
      "train loss:0.130522733635\n",
      "train loss:0.0496865468032\n",
      "train loss:0.11652241791\n",
      "train loss:0.119965028218\n",
      "train loss:0.109288906569\n",
      "train loss:0.113307121233\n",
      "train loss:0.105686596852\n",
      "train loss:0.0882131133126\n",
      "train loss:0.0721914542685\n",
      "train loss:0.100416008715\n",
      "train loss:0.0695002981052\n",
      "train loss:0.094877823134\n",
      "train loss:0.196507971987\n",
      "train loss:0.156563884086\n",
      "train loss:0.146193351439\n",
      "train loss:0.168796952538\n",
      "train loss:0.0607457277684\n",
      "train loss:0.112587090674\n",
      "train loss:0.123203921213\n",
      "train loss:0.091615002442\n",
      "train loss:0.0860677438097\n",
      "train loss:0.0514530117427\n",
      "train loss:0.164291809413\n",
      "train loss:0.0694826332662\n",
      "train loss:0.0380128130786\n",
      "train loss:0.0916269403548\n",
      "train loss:0.140144616992\n",
      "train loss:0.105691835955\n",
      "train loss:0.140823879155\n",
      "train loss:0.107248713613\n",
      "train loss:0.209257388825\n",
      "train loss:0.113280760543\n",
      "train loss:0.0811369770222\n",
      "train loss:0.140937234643\n",
      "train loss:0.0314335852332\n",
      "train loss:0.150509410632\n",
      "train loss:0.0864401511242\n",
      "train loss:0.0963266212206\n",
      "train loss:0.238307303671\n",
      "train loss:0.0971311341143\n",
      "train loss:0.060472474343\n",
      "train loss:0.117786685532\n",
      "train loss:0.091868412663\n",
      "train loss:0.0631254336283\n",
      "train loss:0.272377341108\n",
      "train loss:0.0619242345845\n",
      "=== epoch:2, train acc:0.9653, test acc:0.9666 ===\n",
      "train loss:0.117883175632\n",
      "train loss:0.18859683216\n",
      "train loss:0.102753728727\n",
      "train loss:0.080130039789\n",
      "train loss:0.137895265456\n",
      "train loss:0.096977757785\n",
      "train loss:0.0811640905269\n",
      "train loss:0.0622547175688\n",
      "train loss:0.121893069142\n",
      "train loss:0.0963819952204\n",
      "train loss:0.10474236261\n",
      "train loss:0.161188919993\n",
      "train loss:0.122777103492\n",
      "train loss:0.0616495169952\n",
      "train loss:0.0810853273611\n",
      "train loss:0.0757477344733\n",
      "train loss:0.0927785240543\n",
      "train loss:0.0882878478651\n",
      "train loss:0.131995698122\n",
      "train loss:0.094773155061\n",
      "train loss:0.110079729228\n",
      "train loss:0.111327585437\n",
      "train loss:0.0794955753939\n",
      "train loss:0.16753960265\n",
      "train loss:0.0950942741097\n",
      "train loss:0.0719230389586\n",
      "train loss:0.150431483839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0626098289317\n",
      "train loss:0.0371275892199\n",
      "train loss:0.0924150151615\n",
      "train loss:0.0517047794083\n",
      "train loss:0.0620467856577\n",
      "train loss:0.0894260859319\n",
      "train loss:0.121837163652\n",
      "train loss:0.0459292082591\n",
      "train loss:0.0827573361242\n",
      "train loss:0.113183635642\n",
      "train loss:0.0911672419733\n",
      "train loss:0.0845649579807\n",
      "train loss:0.0824371394498\n",
      "train loss:0.0361734089742\n",
      "train loss:0.0579533933539\n",
      "train loss:0.0953386239217\n",
      "train loss:0.101643138881\n",
      "train loss:0.0622832594689\n",
      "train loss:0.12290055441\n",
      "train loss:0.0447681109428\n",
      "train loss:0.154742296521\n",
      "train loss:0.164039385596\n",
      "train loss:0.101685165265\n",
      "train loss:0.135356735895\n",
      "train loss:0.0630568864589\n",
      "train loss:0.0910515885337\n",
      "train loss:0.0922590902761\n",
      "train loss:0.141124344745\n",
      "train loss:0.0806584856527\n",
      "train loss:0.0894434671813\n",
      "train loss:0.112424847092\n",
      "train loss:0.0970304511056\n",
      "train loss:0.0584499382376\n",
      "train loss:0.0602413526138\n",
      "train loss:0.0719245394966\n",
      "train loss:0.0919545115716\n",
      "train loss:0.125967708262\n",
      "train loss:0.0763956982015\n",
      "train loss:0.102895689714\n",
      "train loss:0.037094517442\n",
      "train loss:0.052089459726\n",
      "train loss:0.0749784020087\n",
      "train loss:0.108913498945\n",
      "train loss:0.0393389674288\n",
      "train loss:0.0435540359702\n",
      "train loss:0.100891790525\n",
      "train loss:0.127275213999\n",
      "train loss:0.0471108904506\n",
      "train loss:0.0874930175522\n",
      "train loss:0.178046485959\n",
      "train loss:0.143323535972\n",
      "train loss:0.0486042611597\n",
      "train loss:0.251829279116\n",
      "train loss:0.0664175783056\n",
      "train loss:0.0375139979246\n",
      "train loss:0.0976414240228\n",
      "train loss:0.0724858704904\n",
      "train loss:0.0673890571985\n",
      "train loss:0.0425748306807\n",
      "train loss:0.139405788358\n",
      "train loss:0.0740121148023\n",
      "train loss:0.0776788036222\n",
      "train loss:0.0715515602622\n",
      "train loss:0.0755799094401\n",
      "train loss:0.108412153857\n",
      "train loss:0.103462175435\n",
      "train loss:0.113408484902\n",
      "train loss:0.0704267130987\n",
      "train loss:0.0955119662214\n",
      "train loss:0.255742691254\n",
      "train loss:0.115250372491\n",
      "train loss:0.11096583277\n",
      "train loss:0.0852411688273\n",
      "train loss:0.0479657601257\n",
      "train loss:0.0521812249162\n",
      "train loss:0.0616902221312\n",
      "train loss:0.0900592956406\n",
      "train loss:0.119895350284\n",
      "train loss:0.0399843582654\n",
      "train loss:0.0939265302866\n",
      "train loss:0.0512362600375\n",
      "train loss:0.238766932998\n",
      "train loss:0.0837832129151\n",
      "train loss:0.0393134734648\n",
      "train loss:0.0958880439948\n",
      "train loss:0.128148174622\n",
      "train loss:0.0448240731417\n",
      "train loss:0.0466230706216\n",
      "train loss:0.0438542877896\n",
      "train loss:0.122341181363\n",
      "train loss:0.0768599944357\n",
      "train loss:0.0390504374418\n",
      "train loss:0.102786130996\n",
      "train loss:0.0845910833475\n",
      "train loss:0.0331454940626\n",
      "train loss:0.17181159691\n",
      "train loss:0.132248400961\n",
      "train loss:0.0918252766926\n",
      "train loss:0.0731606311145\n",
      "train loss:0.0622557338645\n",
      "train loss:0.123595963451\n",
      "train loss:0.0997493361088\n",
      "train loss:0.0892002292319\n",
      "train loss:0.0279476114166\n",
      "train loss:0.118339634899\n",
      "train loss:0.0726373862208\n",
      "train loss:0.0714655524431\n",
      "train loss:0.0766672398496\n",
      "train loss:0.106724120186\n",
      "train loss:0.0723683512633\n",
      "train loss:0.0838394152884\n",
      "train loss:0.0482816809385\n",
      "train loss:0.0811449971809\n",
      "train loss:0.135941854528\n",
      "train loss:0.0630924656777\n",
      "train loss:0.0222205944418\n",
      "train loss:0.0898919632132\n",
      "train loss:0.0785926046699\n",
      "train loss:0.0420830850842\n",
      "train loss:0.0576838440081\n",
      "train loss:0.0478770945721\n",
      "train loss:0.0586670640287\n",
      "train loss:0.184703692275\n",
      "train loss:0.0552993354056\n",
      "train loss:0.157628152474\n",
      "train loss:0.0594529951545\n",
      "train loss:0.0777094228753\n",
      "train loss:0.119421119464\n",
      "train loss:0.0926977518445\n",
      "train loss:0.144528979698\n",
      "train loss:0.102743705762\n",
      "train loss:0.0800565790032\n",
      "train loss:0.0280240679716\n",
      "train loss:0.138163640383\n",
      "train loss:0.0582507547919\n",
      "train loss:0.0595209579566\n",
      "train loss:0.0971928141342\n",
      "train loss:0.105774909713\n",
      "train loss:0.106975333415\n",
      "train loss:0.0329197375156\n",
      "train loss:0.0448973025784\n",
      "train loss:0.0734380963901\n",
      "train loss:0.0813154308767\n",
      "train loss:0.0503949545248\n",
      "train loss:0.094672407492\n",
      "train loss:0.0922802109502\n",
      "train loss:0.0507247996019\n",
      "train loss:0.112179241006\n",
      "train loss:0.0630422667833\n",
      "train loss:0.0710659188091\n",
      "train loss:0.032753649502\n",
      "train loss:0.0920089458191\n",
      "train loss:0.0732969208037\n",
      "train loss:0.0836966812546\n",
      "train loss:0.0299905525761\n",
      "train loss:0.132174070741\n",
      "train loss:0.0919637086529\n",
      "train loss:0.0689498607422\n",
      "train loss:0.175572493838\n",
      "train loss:0.0366776427367\n",
      "train loss:0.0755177981163\n",
      "train loss:0.0609160213015\n",
      "train loss:0.142912395664\n",
      "train loss:0.185525824038\n",
      "train loss:0.0505104221048\n",
      "train loss:0.0712598506723\n",
      "train loss:0.0409070809298\n",
      "train loss:0.0631813982675\n",
      "train loss:0.0763431114248\n",
      "train loss:0.0616250164263\n",
      "train loss:0.0163928489186\n",
      "train loss:0.0953097959794\n",
      "train loss:0.0778154737235\n",
      "train loss:0.0579224306325\n",
      "train loss:0.038072187173\n",
      "train loss:0.135308294834\n",
      "train loss:0.0836053007923\n",
      "train loss:0.0713772401181\n",
      "train loss:0.0849985926484\n",
      "train loss:0.0321578998663\n",
      "train loss:0.119174376948\n",
      "train loss:0.0792070440293\n",
      "train loss:0.0951155846073\n",
      "train loss:0.0570665481711\n",
      "train loss:0.0527061446109\n",
      "train loss:0.137512796232\n",
      "train loss:0.131539382896\n",
      "train loss:0.172088212036\n",
      "train loss:0.113992056436\n",
      "train loss:0.0716890195445\n",
      "train loss:0.0567720588527\n",
      "train loss:0.103393331249\n",
      "train loss:0.086927702768\n",
      "train loss:0.0421352154317\n",
      "train loss:0.0955890670683\n",
      "train loss:0.0456965647202\n",
      "train loss:0.0648867578097\n",
      "train loss:0.165053441021\n",
      "train loss:0.139587946007\n",
      "train loss:0.0669758769599\n",
      "train loss:0.0875089218898\n",
      "train loss:0.0144901815956\n",
      "train loss:0.0535575574616\n",
      "train loss:0.0551147127649\n",
      "train loss:0.0937666939094\n",
      "train loss:0.104757504963\n",
      "train loss:0.106773180426\n",
      "train loss:0.112516925668\n",
      "train loss:0.108335974563\n",
      "train loss:0.0698926467052\n",
      "train loss:0.118949005141\n",
      "train loss:0.059726657506\n",
      "train loss:0.0964304997865\n",
      "train loss:0.111058265\n",
      "train loss:0.0748206787561\n",
      "train loss:0.0640260277814\n",
      "train loss:0.0327184353886\n",
      "train loss:0.074877360405\n",
      "train loss:0.0528917897059\n",
      "train loss:0.158319590148\n",
      "train loss:0.082421780896\n",
      "train loss:0.0260927455179\n",
      "train loss:0.0610733797451\n",
      "train loss:0.154280881073\n",
      "train loss:0.0772504111092\n",
      "train loss:0.143643114566\n",
      "train loss:0.0577182334666\n",
      "train loss:0.0667405230489\n",
      "train loss:0.0438344514117\n",
      "train loss:0.163458716021\n",
      "train loss:0.0599512882693\n",
      "train loss:0.139306771041\n",
      "train loss:0.0772122069268\n",
      "train loss:0.0990076936057\n",
      "train loss:0.0935272135125\n",
      "train loss:0.200855201531\n",
      "train loss:0.0395938483797\n",
      "train loss:0.086668888431\n",
      "train loss:0.1207702305\n",
      "train loss:0.0591440470629\n",
      "train loss:0.0337525664515\n",
      "train loss:0.13557776835\n",
      "train loss:0.0562821649747\n",
      "train loss:0.0802587159028\n",
      "train loss:0.0519480063871\n",
      "train loss:0.122402278218\n",
      "train loss:0.0716629040517\n",
      "train loss:0.0512093851105\n",
      "train loss:0.0492745071186\n",
      "train loss:0.101209093714\n",
      "train loss:0.0809369970681\n",
      "train loss:0.0424290692419\n",
      "train loss:0.0732534077198\n",
      "train loss:0.112394910583\n",
      "train loss:0.0819166310891\n",
      "train loss:0.0401074497625\n",
      "train loss:0.109376307046\n",
      "train loss:0.0450983839744\n",
      "train loss:0.0482365525945\n",
      "train loss:0.0803106994519\n",
      "train loss:0.0438186924077\n",
      "train loss:0.121836736922\n",
      "train loss:0.0947378975878\n",
      "train loss:0.0554975982121\n",
      "train loss:0.114916296365\n",
      "train loss:0.0854503101618\n",
      "train loss:0.117510694038\n",
      "train loss:0.057172713808\n",
      "train loss:0.0682788570973\n",
      "train loss:0.12266425349\n",
      "train loss:0.0454243623761\n",
      "train loss:0.0398783994392\n",
      "train loss:0.114295602266\n",
      "train loss:0.0502788553288\n",
      "train loss:0.0493021399879\n",
      "train loss:0.125291844192\n",
      "train loss:0.0512974087215\n",
      "train loss:0.0793276311227\n",
      "train loss:0.0572639544354\n",
      "train loss:0.141095494057\n",
      "train loss:0.0964200703854\n",
      "train loss:0.0360680234961\n",
      "train loss:0.0659816876049\n",
      "train loss:0.073281480537\n",
      "train loss:0.132298190421\n",
      "train loss:0.0456033503789\n",
      "train loss:0.0852671228589\n",
      "train loss:0.109828120013\n",
      "train loss:0.0315154357096\n",
      "train loss:0.0783946884185\n",
      "train loss:0.0693038507457\n",
      "train loss:0.0979170938973\n",
      "train loss:0.0981952199868\n",
      "train loss:0.0392070047379\n",
      "train loss:0.0744399363922\n",
      "train loss:0.103253999452\n",
      "train loss:0.0901373055743\n",
      "train loss:0.118437460642\n",
      "train loss:0.11533191851\n",
      "train loss:0.0773944881113\n",
      "train loss:0.0460031263512\n",
      "train loss:0.0744318253563\n",
      "train loss:0.0290063081512\n",
      "train loss:0.100140160252\n",
      "train loss:0.0539046625802\n",
      "train loss:0.0435638190757\n",
      "train loss:0.0578954903156\n",
      "train loss:0.0625160448206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0597686631784\n",
      "train loss:0.0649341576539\n",
      "train loss:0.100428468682\n",
      "train loss:0.142773769029\n",
      "train loss:0.131321720165\n",
      "train loss:0.0768687029047\n",
      "train loss:0.114493119218\n",
      "train loss:0.0950633801436\n",
      "train loss:0.0875584419048\n",
      "train loss:0.0252048972341\n",
      "train loss:0.0640612531768\n",
      "train loss:0.128714164613\n",
      "train loss:0.0632258141598\n",
      "train loss:0.0485846617829\n",
      "train loss:0.0566776251632\n",
      "train loss:0.0858233544128\n",
      "train loss:0.0239264296562\n",
      "train loss:0.0577260494463\n",
      "train loss:0.0777789402926\n",
      "train loss:0.143310166088\n",
      "train loss:0.128093068516\n",
      "train loss:0.0373486468777\n",
      "train loss:0.125765126419\n",
      "train loss:0.0621839338081\n",
      "train loss:0.1243804289\n",
      "train loss:0.12786460834\n",
      "train loss:0.0811545093781\n",
      "train loss:0.120433369374\n",
      "train loss:0.111231677249\n",
      "train loss:0.0672763476639\n",
      "train loss:0.040433112553\n",
      "train loss:0.0329988762872\n",
      "train loss:0.0270473223679\n",
      "train loss:0.106884196812\n",
      "train loss:0.150708236976\n",
      "train loss:0.0463509608825\n",
      "train loss:0.111519430257\n",
      "train loss:0.0239324465356\n",
      "train loss:0.0628474054118\n",
      "train loss:0.0342422099414\n",
      "train loss:0.0510705790205\n",
      "train loss:0.0624426160189\n",
      "train loss:0.030679031528\n",
      "train loss:0.0607728662984\n",
      "train loss:0.0213773588018\n",
      "train loss:0.0964556596643\n",
      "train loss:0.0627522759211\n",
      "train loss:0.0472983417312\n",
      "train loss:0.058050558172\n",
      "train loss:0.113171311155\n",
      "train loss:0.0702165059003\n",
      "train loss:0.0772887819349\n",
      "train loss:0.103052536612\n",
      "train loss:0.186202023332\n",
      "train loss:0.0802814187349\n",
      "train loss:0.0464466587471\n",
      "train loss:0.055615951686\n",
      "train loss:0.0264761517077\n",
      "train loss:0.0787659331824\n",
      "train loss:0.072210878048\n",
      "train loss:0.0561258373932\n",
      "train loss:0.0662334094465\n",
      "train loss:0.0556661194924\n",
      "train loss:0.0112141859585\n",
      "train loss:0.0398123338741\n",
      "train loss:0.06591621446\n",
      "train loss:0.0684568104278\n",
      "train loss:0.0389952174744\n",
      "train loss:0.0795494495491\n",
      "train loss:0.0493983558827\n",
      "train loss:0.0650392473873\n",
      "train loss:0.053893606143\n",
      "train loss:0.0630835604553\n",
      "train loss:0.0313913042867\n",
      "train loss:0.144684513823\n",
      "train loss:0.0708439663598\n",
      "train loss:0.0725264990247\n",
      "train loss:0.0892670726716\n",
      "train loss:0.0809422193494\n",
      "train loss:0.136098474044\n",
      "train loss:0.0874544310195\n",
      "train loss:0.0344203284769\n",
      "train loss:0.0339111891201\n",
      "train loss:0.0853266748803\n",
      "train loss:0.0190486843777\n",
      "train loss:0.0504494832904\n",
      "train loss:0.040860322497\n",
      "train loss:0.122630484772\n",
      "train loss:0.082206008353\n",
      "train loss:0.069968240927\n",
      "train loss:0.0736797228176\n",
      "train loss:0.0459256803207\n",
      "train loss:0.0388451358037\n",
      "train loss:0.0442827488259\n",
      "train loss:0.133766440192\n",
      "train loss:0.106749576709\n",
      "train loss:0.0790549887954\n",
      "train loss:0.0307774157725\n",
      "train loss:0.0919582077166\n",
      "train loss:0.0298532841458\n",
      "train loss:0.0257642785249\n",
      "train loss:0.188921216315\n",
      "train loss:0.0319736419051\n",
      "train loss:0.0204615871512\n",
      "train loss:0.0270010036909\n",
      "train loss:0.0323332935532\n",
      "train loss:0.11539145442\n",
      "train loss:0.139227958004\n",
      "train loss:0.0481542311213\n",
      "train loss:0.043095943627\n",
      "train loss:0.145442234199\n",
      "train loss:0.062806057796\n",
      "train loss:0.0370594833247\n",
      "train loss:0.0322028023141\n",
      "train loss:0.0290078555079\n",
      "train loss:0.0680608569091\n",
      "train loss:0.144397630604\n",
      "train loss:0.0312974332753\n",
      "train loss:0.0242234135247\n",
      "train loss:0.0678236560235\n",
      "train loss:0.0223354240165\n",
      "train loss:0.113022956524\n",
      "train loss:0.0495236892512\n",
      "train loss:0.0609366836376\n",
      "train loss:0.0246398383588\n",
      "train loss:0.0946930369382\n",
      "train loss:0.0739727268752\n",
      "train loss:0.169695475845\n",
      "train loss:0.111069219718\n",
      "train loss:0.0347965178592\n",
      "train loss:0.0409596863854\n",
      "train loss:0.0222522283831\n",
      "train loss:0.0354974711272\n",
      "train loss:0.0866052363582\n",
      "train loss:0.0402099301682\n",
      "train loss:0.0368795651293\n",
      "train loss:0.0429352813362\n",
      "train loss:0.0380403986395\n",
      "train loss:0.0802037128269\n",
      "train loss:0.107477056489\n",
      "train loss:0.028664189712\n",
      "train loss:0.0573015324559\n",
      "train loss:0.0638916741387\n",
      "train loss:0.0448793035903\n",
      "train loss:0.158909704808\n",
      "train loss:0.0631681443122\n",
      "train loss:0.0274002354819\n",
      "train loss:0.0314042924198\n",
      "train loss:0.0412922073282\n",
      "train loss:0.0890815573651\n",
      "train loss:0.0647362556787\n",
      "train loss:0.0560026864067\n",
      "train loss:0.032413642443\n",
      "train loss:0.0216255970562\n",
      "train loss:0.0721469588861\n",
      "train loss:0.0305904398708\n",
      "train loss:0.0375248747132\n",
      "train loss:0.0236689287439\n",
      "train loss:0.0257072603861\n",
      "train loss:0.126950804687\n",
      "train loss:0.0183715219807\n",
      "train loss:0.0538396446227\n",
      "train loss:0.133633692555\n",
      "train loss:0.0427755544311\n",
      "train loss:0.0249277693684\n",
      "train loss:0.0515333636996\n",
      "train loss:0.120217429377\n",
      "train loss:0.0841710031205\n",
      "train loss:0.0388707841445\n",
      "train loss:0.153838917786\n",
      "train loss:0.0620161369514\n",
      "train loss:0.0817664065298\n",
      "train loss:0.049707819073\n",
      "train loss:0.0441797444621\n",
      "train loss:0.0908359120746\n",
      "train loss:0.114055554817\n",
      "train loss:0.0437819506768\n",
      "train loss:0.0284009836065\n",
      "train loss:0.0726398523013\n",
      "train loss:0.0778539492891\n",
      "train loss:0.0545336709751\n",
      "train loss:0.0260188900121\n",
      "train loss:0.0830791426455\n",
      "train loss:0.0241684165077\n",
      "train loss:0.032475356282\n",
      "train loss:0.0480828755155\n",
      "train loss:0.109815510386\n",
      "train loss:0.0519377889632\n",
      "train loss:0.0814501232278\n",
      "train loss:0.0902839613242\n",
      "train loss:0.0443051192491\n",
      "train loss:0.0792683295926\n",
      "train loss:0.0576172510217\n",
      "train loss:0.0312253584756\n",
      "train loss:0.0953701464707\n",
      "train loss:0.0377385883223\n",
      "train loss:0.0975965629889\n",
      "train loss:0.0437084357566\n",
      "train loss:0.0866042248097\n",
      "train loss:0.0551208172667\n",
      "train loss:0.0429095572642\n",
      "train loss:0.0866304365576\n",
      "train loss:0.0958258734557\n",
      "train loss:0.143574722818\n",
      "train loss:0.114395066244\n",
      "train loss:0.0449106002755\n",
      "train loss:0.0282404445975\n",
      "train loss:0.11173962789\n",
      "train loss:0.0846338262557\n",
      "train loss:0.0446600158578\n",
      "train loss:0.0655815865366\n",
      "train loss:0.108135784095\n",
      "train loss:0.0748522165539\n",
      "train loss:0.0761877134981\n",
      "train loss:0.0721170001429\n",
      "train loss:0.0652111738837\n",
      "train loss:0.0648756996727\n",
      "train loss:0.047284731665\n",
      "train loss:0.0833461143168\n",
      "train loss:0.0397624305221\n",
      "train loss:0.0320414460574\n",
      "train loss:0.0482930751309\n",
      "train loss:0.158949677941\n",
      "train loss:0.03671696097\n",
      "train loss:0.0476773172261\n",
      "train loss:0.00925973231222\n",
      "train loss:0.0374083808949\n",
      "train loss:0.0492604587314\n",
      "train loss:0.0530789217867\n",
      "train loss:0.0625174770774\n",
      "train loss:0.0309545298946\n",
      "train loss:0.0383496278589\n",
      "train loss:0.0787393298575\n",
      "train loss:0.0438140622519\n",
      "train loss:0.0720157021559\n",
      "train loss:0.137700941103\n",
      "train loss:0.0400229307218\n",
      "train loss:0.0328194581568\n",
      "train loss:0.0488377660378\n",
      "train loss:0.0721540632573\n",
      "train loss:0.0193958280625\n",
      "train loss:0.0309620574859\n",
      "train loss:0.0666944383533\n",
      "train loss:0.0420672599341\n",
      "train loss:0.0498278925026\n",
      "train loss:0.0190881622146\n",
      "train loss:0.201863906704\n",
      "train loss:0.020480128153\n",
      "train loss:0.148854597095\n",
      "train loss:0.0689552552505\n",
      "train loss:0.033718891603\n",
      "train loss:0.0575292492132\n",
      "train loss:0.091507621852\n",
      "train loss:0.0963230491653\n",
      "train loss:0.0415872682091\n",
      "train loss:0.0506360305517\n",
      "train loss:0.0454639315515\n",
      "train loss:0.0889897549893\n",
      "train loss:0.0429916639834\n",
      "train loss:0.0318837888481\n",
      "train loss:0.0606886069822\n",
      "train loss:0.0203005416523\n",
      "train loss:0.0622270612159\n",
      "train loss:0.0217458157538\n",
      "train loss:0.0285340444832\n",
      "=== epoch:3, train acc:0.9786, test acc:0.9788 ===\n",
      "train loss:0.128591504102\n",
      "train loss:0.0394770045673\n",
      "train loss:0.0960457013833\n",
      "train loss:0.0692868997413\n",
      "train loss:0.0845753453657\n",
      "train loss:0.0542799526882\n",
      "train loss:0.0660336073119\n",
      "train loss:0.0669392090227\n",
      "train loss:0.0244612271036\n",
      "train loss:0.0589436045859\n",
      "train loss:0.0219749803165\n",
      "train loss:0.0343738763264\n",
      "train loss:0.0201383164941\n",
      "train loss:0.0436894618552\n",
      "train loss:0.0527333940108\n",
      "train loss:0.123522776893\n",
      "train loss:0.0221378935628\n",
      "train loss:0.031859633347\n",
      "train loss:0.028429785201\n",
      "train loss:0.0288925425991\n",
      "train loss:0.0702532797351\n",
      "train loss:0.0319072849283\n",
      "train loss:0.0246140979302\n",
      "train loss:0.0628324825413\n",
      "train loss:0.0517475480129\n",
      "train loss:0.0659117181156\n",
      "train loss:0.0330749216966\n",
      "train loss:0.122103117078\n",
      "train loss:0.0205546875652\n",
      "train loss:0.0676732806526\n",
      "train loss:0.0292490102915\n",
      "train loss:0.095325549011\n",
      "train loss:0.0630317588824\n",
      "train loss:0.0460033621697\n",
      "train loss:0.0458927320142\n",
      "train loss:0.0363854779909\n",
      "train loss:0.0271516392865\n",
      "train loss:0.107538068214\n",
      "train loss:0.0144490776104\n",
      "train loss:0.0489734686864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0518053195019\n",
      "train loss:0.0504815479835\n",
      "train loss:0.0381459757959\n",
      "train loss:0.030739536409\n",
      "train loss:0.0639753752112\n",
      "train loss:0.0190385824003\n",
      "train loss:0.0915467942565\n",
      "train loss:0.0719722558435\n",
      "train loss:0.058730664442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-a7384927d3c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                  evaluate_sample_num_per_epoch=10000)\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# 매개변수 보존\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\deeplearning\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\deeplearning\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-862511f7997e>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0m정답\u001b[0m \u001b[0m레이블\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-862511f7997e>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\deeplearning\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\deeplearning\\common\\util.py\u001b[0m in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mstride\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m   1292\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`pad_width` must be of integral type.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1294\u001b[1;33m     \u001b[0mnarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1295\u001b[0m     \u001b[0mpad_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28),\n",
    "                       conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                       hidden_size=100,output_size=10,weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=max_epochs, mini_batch_size=100,\n",
    "                 optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                 evaluate_sample_num_per_epoch=10000)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
